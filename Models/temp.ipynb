{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3, 5 ,8]\n",
    "print(a[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a():\n",
    "    for i in range(100):\n",
    "        yield 1\n",
    "itr = a()\n",
    "print(next(itr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itr.__new__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class InceptionModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_pool):\n",
    "        super(InceptionModule, self).__init__()\n",
    "\n",
    "        # 1x1 convolution branch\n",
    "        self.branch1x1 = nn.Conv2d(in_channels, out_1x1, kernel_size=1)\n",
    "\n",
    "        # 3x3 convolution branch\n",
    "        self.branch3x3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, red_3x3, kernel_size=1),\n",
    "            nn.Conv2d(red_3x3, out_3x3, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        # 5x5 convolution branch\n",
    "        self.branch5x5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, red_5x5, kernel_size=1),\n",
    "            nn.Conv2d(red_5x5, out_5x5, kernel_size=5, padding=2)\n",
    "        )\n",
    "\n",
    "        # Max pooling branch\n",
    "        self.branch_pool = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels, out_pool, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through each branch and concatenate the outputs\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "        branch3x3 = self.branch3x3(x)\n",
    "        branch5x5 = self.branch5x5(x)\n",
    "        branch_pool = self.branch_pool(x)\n",
    "\n",
    "        # Concatenate along the channel dimension\n",
    "        outputs = [branch1x1, branch3x3, branch5x5, branch_pool]\n",
    "        op = torch.cat(outputs, 1)\n",
    "        print(op.shape)\n",
    "        return op\n",
    "    \n",
    "\n",
    "# Example usage:\n",
    "in_channels = 3  # Input channels (e.g., for an RGB image)\n",
    "out_1x1 = 64\n",
    "red_3x3 = 128\n",
    "out_3x3 = 192\n",
    "red_5x5 = 32\n",
    "out_5x5 = 96\n",
    "out_pool = 64\n",
    "\n",
    "inception_module = InceptionModule(in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_pool)\n",
    "\n",
    "# Example input tensor\n",
    "input_tensor = torch.randn(1, in_channels, 224, 224)\n",
    "\n",
    "# Forward pass through the Inception module\n",
    "output_tensor = inception_module(input_tensor)\n",
    "\n",
    "print(\"Input shape:\", input_tensor.shape)\n",
    "print(\"Output shape:\", output_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class InceptionModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_pool, fc_units):\n",
    "        super(InceptionModule, self).__init__()\n",
    "\n",
    "        # 1x1 convolution branch\n",
    "        self.branch1x1 = nn.Conv2d(in_channels, out_1x1, kernel_size=1)\n",
    "\n",
    "        # 3x3 convolution branch\n",
    "        self.branch3x3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, red_3x3, kernel_size=1),\n",
    "            nn.Conv2d(red_3x3, out_3x3, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        # 5x5 convolution branch\n",
    "        self.branch5x5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, red_5x5, kernel_size=1),\n",
    "            nn.Conv2d(red_5x5, out_5x5, kernel_size=5, padding=2)\n",
    "        )\n",
    "\n",
    "        # Max pooling branch\n",
    "        self.branch_pool = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels, out_pool, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(out_1x1 + out_3x3 + out_5x5 + out_pool, fc_units)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through each branch\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "        branch3x3 = self.branch3x3(x)\n",
    "        branch5x5 = self.branch5x5(x)\n",
    "        branch_pool = self.branch_pool(x)\n",
    "\n",
    "        # Concatenate along the channel dimension\n",
    "        concat_features = torch.cat([branch1x1, branch3x3, branch5x5, branch_pool], 1)\n",
    "        print(concat_features.shape)\n",
    "        # Flatten the features before passing through the fully connected layer\n",
    "        flat_features = concat_features.view(concat_features.size(0), -1)\n",
    "        flat_features1 = torch.flatten(concat_features, 1)\n",
    "        flat_features1 = flat_features[np.newaxis, :]\n",
    "        print(flat_features.shape)\n",
    "        print(flat_features1.shape)\n",
    "\n",
    "        # Fully connected layer\n",
    "        fc_output = self.fc(flat_features)\n",
    "\n",
    "        return fc_output\n",
    "\n",
    "# Example usage:\n",
    "in_channels = 3  # Input channels (e.g., for an RGB image)\n",
    "out_1x1 = 64\n",
    "red_3x3 = 128\n",
    "out_3x3 = 192\n",
    "red_5x5 = 32\n",
    "out_5x5 = 96\n",
    "out_pool = 64\n",
    "fc_units = 256  # Number of units in the fully connected layer\n",
    "\n",
    "inception_module = InceptionModule(in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_pool, fc_units)\n",
    "\n",
    "# Example input tensor\n",
    "input_tensor = torch.randn(1, in_channels, 224, 224)\n",
    "\n",
    "# Forward pass through the Inception module\n",
    "output_tensor = inception_module(input_tensor)\n",
    "\n",
    "print(\"Input shape:\", input_tensor.shape)\n",
    "print(\"Output shape:\", output_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "tensor = torch.randn(2, 3, 4, 2)\n",
    "print(tensor.size())\n",
    "print(tensor.view(3, -1))\n",
    "print(tensor.view(3, -1).shape)\n",
    "print(torch.flatten(tensor, 2))\n",
    "print(torch.flatten(tensor, 2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image, ImageReadMode\n",
    "import numpy as np\n",
    "a = read_image(\"M:/2023_Predator_Alternative_Wallpaper_A New Beginning_3840x2160.png\", ImageReadMode.RGB)\n",
    "print(a.shape)\n",
    "a = a.permute(1, 2, 0)\n",
    "\n",
    "print(a.shape)\n",
    "a = np.array(a)\n",
    "import cv2\n",
    "a = cv2.cvtColor(a, cv2.COLOR_RGB2BGR)\n",
    "a = cv2.resize(a, (400, 400))\n",
    "cv2.imshow(\"img\", a)\n",
    "cv2.waitKey(1000)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "from PIL import Image\n",
    "a = Image.open(\"M:/2023_Predator_Alternative_Wallpaper_A New Beginning_3840x2160.png\")\n",
    "a = a.convert(\"RGB\")\n",
    "display(a)\n",
    "print(a.size)\n",
    "\n",
    "\n",
    "\n",
    "a = cv2.imread(\"M:/2023_Predator_Alternative_Wallpaper_A New Beginning_3840x2160.png\")\n",
    "print(a.shape)\n",
    "print(a.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "a = torch.randn(100, 200, 3)\n",
    "# print(a)\n",
    "a = a.permute(2, 0, 1)\n",
    "print(a.shape)\n",
    "print(a)\n",
    "a = a.permute(1, 2, 0)\n",
    "a = np.array(a)\n",
    "cv2.imshow(\"img\", a)\n",
    "cv2.waitKey(1000)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = read_image(\"M:/Datasets/FaceRecognition/labels-5/100016/0.png\")\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.shape)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"W:\\Projects\\FaceRecognition\\Models\\Data\\Dataset.csv\")\n",
    "bs = 12\n",
    "b = 8\n",
    "position = bs*(b-1)\n",
    "data.iloc[position:position+bs, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"W:\\Projects\\FaceRecognition\\Models\\Data\\Dataset.csv\")\n",
    "bs = 12\n",
    "b = 8\n",
    "position = bs*(b-1)\n",
    "data = data.iloc[position:position+bs, :]\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"W:\\Projects\\FaceRecognition\\Models\\Data\\Dataset.csv\")\n",
    "bs = 12\n",
    "b = 8\n",
    "position = bs*(b-1)\n",
    "data.iloc[position:position+bs, :]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"W:\\Projects\\FaceRecognition\\Models\\Data\\Dataset.csv\")\n",
    "bs = 12\n",
    "b = 8\n",
    "position = bs*(b-1)\n",
    "data = data.iloc[position:position+bs, :]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[0]['Anchor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"W:\\Projects\\FaceRecognition\\Models\\Data\\Dataset.csv\")\n",
    "bs = 12\n",
    "b = 8\n",
    "position = bs*(b-1)\n",
    "data = data.iloc[position:position+bs, :]\n",
    "data.head()\n",
    "for i in range(len(data)):\n",
    "    print(i)\n",
    "for i in data:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    images = []\n",
    "    for j, _ in enumerate(data):\n",
    "        print(data.iloc[i, j])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "import numpy as np\n",
    "a = [1, 250]\n",
    "a = read_image(\"M:/Datasets/FaceRecognition/labels-5/100000/1.png\", ImageReadMode.RGB)\n",
    "# a = a.transpose()\n",
    "a = torch.transpose(a, 0, 1)\n",
    "a = torch.transpose(a, 1, 2)\n",
    "print(a.shape)\n",
    "# break\n",
    "a = np.array(a)\n",
    "import cv2\n",
    "cv2.imshow(\"img\", a)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "# print(cv2.imread(\"M:/Datasets/FaceRecognition/labels-5/100000/1.png\").shape)\n",
    "# print(torch.divide(a[0][0][0], 255, ))\n",
    "\n",
    "# a = torch.divide(torch.asarray(a, dtype = torch.float64), 255)\n",
    "# a = a/255.0\n",
    "# print(a.dtype)\n",
    "torch.set_printoptions(profile = \"full\")\n",
    "print(a.shape)\n",
    "print(a)\n",
    "# print(torch.asarray(a, dtype=torch.float64)/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.clamp(torch.as_tensor([5, 0]), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anchor</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W:/Projects/FaceRecognition/Models/Datasets/Fa...</td>\n",
       "      <td>W:/Projects/FaceRecognition/Models/Datasets/Fa...</td>\n",
       "      <td>W:/Projects/FaceRecognition/Models/Datasets/Fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>W:/Projects/FaceRecognition/Models/Datasets/Fa...</td>\n",
       "      <td>W:/Projects/FaceRecognition/Models/Datasets/Fa...</td>\n",
       "      <td>W:/Projects/FaceRecognition/Models/Datasets/Fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>W:/Projects/FaceRecognition/Models/Datasets/Fa...</td>\n",
       "      <td>W:/Projects/FaceRecognition/Models/Datasets/Fa...</td>\n",
       "      <td>W:/Projects/FaceRecognition/Models/Datasets/Fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>W:/Projects/FaceRecognition/Models/Datasets/Fa...</td>\n",
       "      <td>W:/Projects/FaceRecognition/Models/Datasets/Fa...</td>\n",
       "      <td>W:/Projects/FaceRecognition/Models/Datasets/Fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>W:/Projects/FaceRecognition/Models/Datasets/Fa...</td>\n",
       "      <td>W:/Projects/FaceRecognition/Models/Datasets/Fa...</td>\n",
       "      <td>W:/Projects/FaceRecognition/Models/Datasets/Fa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Anchor  \\\n",
       "0  W:/Projects/FaceRecognition/Models/Datasets/Fa...   \n",
       "1  W:/Projects/FaceRecognition/Models/Datasets/Fa...   \n",
       "2  W:/Projects/FaceRecognition/Models/Datasets/Fa...   \n",
       "3  W:/Projects/FaceRecognition/Models/Datasets/Fa...   \n",
       "4  W:/Projects/FaceRecognition/Models/Datasets/Fa...   \n",
       "\n",
       "                                            Positive  \\\n",
       "0  W:/Projects/FaceRecognition/Models/Datasets/Fa...   \n",
       "1  W:/Projects/FaceRecognition/Models/Datasets/Fa...   \n",
       "2  W:/Projects/FaceRecognition/Models/Datasets/Fa...   \n",
       "3  W:/Projects/FaceRecognition/Models/Datasets/Fa...   \n",
       "4  W:/Projects/FaceRecognition/Models/Datasets/Fa...   \n",
       "\n",
       "                                            Negative  \n",
       "0  W:/Projects/FaceRecognition/Models/Datasets/Fa...  \n",
       "1  W:/Projects/FaceRecognition/Models/Datasets/Fa...  \n",
       "2  W:/Projects/FaceRecognition/Models/Datasets/Fa...  \n",
       "3  W:/Projects/FaceRecognition/Models/Datasets/Fa...  \n",
       "4  W:/Projects/FaceRecognition/Models/Datasets/Fa...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"Data/Dataset.csv\"\n",
    "import pandas as pd\n",
    "data = pd.read_csv(path)\n",
    "import numpy as np\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77000\n",
      "77000\n",
      "[['W:/Projects/FaceRecognition/Models/Datasets/FaceRecognition/labels-5/100000/0.png'\n",
      "  'W:/Projects/FaceRecognition/Models/Datasets/FaceRecognition/labels-5/100000/5.png'\n",
      "  'W:/Projects/FaceRecognition/Models/Datasets/FaceRecognition/labels-5/114982/0.png']\n",
      " ['W:/Projects/FaceRecognition/Models/Datasets/FaceRecognition/labels-5/100000/0.png'\n",
      "  'W:/Projects/FaceRecognition/Models/Datasets/FaceRecognition/labels-5/100000/2.png'\n",
      "  'W:/Projects/FaceRecognition/Models/Datasets/FaceRecognition/labels-5/102712/1.png']\n",
      " ['W:/Projects/FaceRecognition/Models/Datasets/FaceRecognition/labels-5/100000/0.png'\n",
      "  'W:/Projects/FaceRecognition/Models/Datasets/FaceRecognition/labels-5/100000/3.png'\n",
      "  'W:/Projects/FaceRecognition/Models/Datasets/FaceRecognition/labels-5/122622/5.png']\n",
      " ...\n",
      " ['W:/Projects/FaceRecognition/Models/Datasets/FaceRecognition/labels-72/1898/0.png'\n",
      "  'W:/Projects/FaceRecognition/Models/Datasets/FaceRecognition/labels-72/1898/54.png'\n",
      "  'W:/Projects/FaceRecognition/Models/Datasets/FaceRecognition/labels-72/783/44.png']\n",
      " ['W:/Projects/FaceRecognition/Models/Datasets/FaceRecognition/labels-72/1898/0.png'\n",
      "  'W:/Projects/FaceRecognition/Models/Datasets/FaceRecognition/labels-72/1898/65.png'\n",
      "  'W:/Projects/FaceRecognition/Models/Datasets/FaceRecognition/labels-72/1029/1.png']\n",
      " ['W:/Projects/FaceRecognition/Models/Datasets/FaceRecognition/labels-72/1898/0.png'\n",
      "  'W:/Projects/FaceRecognition/Models/Datasets/FaceRecognition/labels-72/1898/48.png'\n",
      "  'W:/Projects/FaceRecognition/Models/Datasets/FaceRecognition/labels-72/18/14.png']]\n"
     ]
    }
   ],
   "source": [
    "data = data.values\n",
    "print(data.shape[0])\n",
    "print(len(data))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "a, b = train_test_split(data, test_size=0.2, shuffle=False)\n",
    "print(a.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18.0000])\n",
      "tensor([10.3923])\n",
      "tensor([8.6535])\n",
      "tensor([7.8964])\n",
      "tensor([7.4744])\n",
      "\n",
      "\n",
      "tensor([4.0000])\n",
      "tensor([2.4495])\n",
      "tensor([2.1544])\n",
      "tensor([2.0598])\n",
      "tensor([2.0244])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming x and y are tensors representing two sets of points\n",
    "x = torch.tensor([[1.0, 2.0, 3.0]])\n",
    "y = torch.tensor([[7.0, 8.0, 9.0]])\n",
    "z = torch.tensor([[3.0, 1.0, 4.0]])\n",
    "\n",
    "# Compute Euclidean distance (equivalent to pairwise_distance with p=2)\n",
    "distance = F.pairwise_distance(x, y, p=1)\n",
    "print(distance)\n",
    "distance = F.pairwise_distance(x, y, p=2)\n",
    "print(distance)\n",
    "distance = F.pairwise_distance(x, y, p=3)\n",
    "print(distance)\n",
    "distance = F.pairwise_distance(x, y, p=4)\n",
    "print(distance)\n",
    "distance = F.pairwise_distance(x, y, p=5)\n",
    "print(distance)\n",
    "print(\"\\n\")\n",
    "distance = F.pairwise_distance(x, z, p=1)\n",
    "print(distance)\n",
    "distance = F.pairwise_distance(x, z, p=2)\n",
    "print(distance)\n",
    "distance = F.pairwise_distance(x, z, p=3)\n",
    "print(distance)\n",
    "distance = F.pairwise_distance(x, z, p=4)\n",
    "print(distance)\n",
    "distance = F.pairwise_distance(x, z, p=5)\n",
    "print(distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch import optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_channels, embedding_size, batch_size, p_dropout, p_linear_dropout):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv = nn.Sequential(nn.Conv2d(in_channels, in_channels, kernel_size=1, padding=0),\n",
    "                                   nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "\n",
    "        self.b1 = nn.Conv2d(3, 64, kernel_size=1, padding=\"same\")\n",
    "        self.b2 = nn.Sequential(nn.Conv2d(in_channels, 92, kernel_size=1, padding=\"same\"),\n",
    "                                nn.Conv2d(92, 128, kernel_size=3, padding=\"same\"))\n",
    "        self.b3 = nn.Sequential(nn.Conv2d(in_channels, 24, kernel_size=1, padding=\"same\"),\n",
    "                                nn.Conv2d(24, 32, kernel_size=5, padding=\"same\"))\n",
    "        self.b4 = nn.Sequential(nn.MaxPool2d(kernel_size=3, padding=1, stride = 1),\n",
    "                                nn.Conv2d(in_channels, 32, kernel_size=1, padding=\"same\"))\n",
    "        \n",
    "\n",
    "        self.b5 = nn.Conv2d(256, 32, kernel_size=1, padding=\"same\")\n",
    "        self.b6 = nn.Sequential(nn.Conv2d(256, 128, kernel_size=1, padding=\"same\"),\n",
    "                                nn.Conv2d(128, 64, kernel_size=3, padding=\"same\"))\n",
    "        self.b7 = nn.Sequential(nn.Conv2d(256, 24, kernel_size=1, padding=\"same\"),\n",
    "                                nn.Conv2d(24, 16, kernel_size=5, padding=\"same\"))\n",
    "        self.b8 = nn.Sequential(nn.MaxPool2d(kernel_size=3, padding=1, stride = 1),\n",
    "                                nn.Conv2d(256, 16, kernel_size=1, padding=\"same\"))\n",
    "\n",
    "        self.dropout = nn.Dropout2d(p_dropout)\n",
    "\n",
    "        self.b5 = nn.Conv2d(256, 32, kernel_size=1, padding=\"same\")\n",
    "        self.b6 = nn.Sequential(nn.Conv2d(256, 128, kernel_size=1, padding=\"same\"),\n",
    "                                nn.Conv2d(128, 64, kernel_size=3, padding=\"same\"))\n",
    "        self.b7 = nn.Sequential(nn.Conv2d(256, 24, kernel_size=1, padding=\"same\"),\n",
    "                                nn.Conv2d(24, 16, kernel_size=5, padding=\"same\"))\n",
    "        self.b8 = nn.Sequential(nn.MaxPool2d(kernel_size=3, padding=1, stride = 1),\n",
    "                                nn.Conv2d(256, 16, kernel_size=1, padding=\"same\"))\n",
    "        \n",
    "        self.fc = nn.Sequential(nn.Linear(401408, 512, bias=True),\n",
    "                                nn.Dropout(p_linear_dropout),\n",
    "                                nn.Linear(512, embedding_size, bias=True))\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "\n",
    "        xb1 = self.b1(x)\n",
    "        xb2 = self.b2(x)\n",
    "        xb3 = self.b3(x)\n",
    "        xb4 = self.b4(x)\n",
    "        x = torch.cat([xb1, xb2, xb3, xb4], dim = 1)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        xb5 = self.b5(x)\n",
    "        xb6 = self.b6(x)\n",
    "        xb7 = self.b7(x)\n",
    "        xb8 = self.b8(x)\n",
    "        x = torch.cat([xb5, xb6, xb7, xb8], dim = 1)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        \n",
    "        x = x.view(self.batch_size, -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0239,  0.0732,  0.1490,  0.0123,  0.0208,  0.0492, -0.1234, -0.0815,\n",
      "          0.1561,  0.0435, -0.0483,  0.0423, -0.0674, -0.0273,  0.0601,  0.1457,\n",
      "         -0.1493,  0.0574,  0.0187,  0.1827,  0.0406,  0.1396,  0.0131,  0.0534,\n",
      "         -0.1483,  0.0182, -0.1108, -0.0984, -0.0748,  0.0761, -0.0199, -0.1126,\n",
      "         -0.1302,  0.1328, -0.1969, -0.0453, -0.0492,  0.0248, -0.1084, -0.1192,\n",
      "          0.1149,  0.0241, -0.1722,  0.0532,  0.0424, -0.0042, -0.0999,  0.1342,\n",
      "          0.1198, -0.0740,  0.0639,  0.0794,  0.0367, -0.0343, -0.0472,  0.1189,\n",
      "         -0.0551,  0.1241,  0.0905, -0.0604,  0.0863, -0.0172,  0.0117, -0.1287,\n",
      "          0.0582, -0.0742, -0.0392,  0.1068, -0.0021,  0.0420, -0.1933, -0.0401,\n",
      "          0.1983,  0.0848, -0.0293,  0.2292,  0.1509, -0.1337,  0.0344, -0.2019,\n",
      "          0.1708, -0.0746,  0.1380, -0.0310,  0.0189,  0.1341, -0.2015, -0.0515,\n",
      "          0.1205, -0.0339, -0.2414, -0.2466,  0.0597,  0.0040, -0.0242, -0.1615,\n",
      "          0.0313,  0.0380, -0.2106,  0.0041, -0.1925, -0.2952,  0.1666, -0.0111,\n",
      "          0.0981, -0.0964, -0.0976, -0.1768,  0.0811,  0.1702, -0.1043,  0.0830,\n",
      "         -0.0797, -0.1086, -0.1056, -0.0959, -0.2037, -0.0706,  0.0477,  0.0699,\n",
      "          0.0476,  0.1823,  0.0208,  0.1367, -0.2662,  0.0176, -0.3004,  0.1198]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming your model expects input images with 3 channels and size 256x256\n",
    "batch_size = 1  # You can adjust the batch size\n",
    "in_channels = 3\n",
    "image_size = (112, 112)\n",
    "\n",
    "# Create a random input tensor\n",
    "sample_input = torch.rand((batch_size, in_channels, *image_size))\n",
    "\n",
    "# Instantiate your model\n",
    "embedding_size = 128  # You can adjust the embedding size\n",
    "p_dropout = 0.5\n",
    "p_linear_dropout = 0.3\n",
    "model = Model(in_channels, embedding_size, batch_size, p_dropout, p_linear_dropout)\n",
    "\n",
    "# Pass the input through the model\n",
    "output = model(sample_input)\n",
    "print(output)\n",
    "# Print the output shape\n",
    "print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10.3923]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming x and y are tensors representing two sets of points\n",
    "x = torch.tensor([[1.0, 2.0, 3.0]])\n",
    "y = torch.tensor([[7.0, 8.0, 9.0]])\n",
    "\n",
    "# Compute Minkowski distance with p=2 (equivalent to Euclidean distance)\n",
    "distance = torch.cdist(x, y, p=2)\n",
    "print(distance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Get the current device\n",
    "current_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print the current device\n",
    "print(\"Current Device:\", current_device)\n",
    "\n",
    "# Create a tensor on the current device\n",
    "tensor_on_current_device = torch.tensor([1, 2, 3]).to(current_device)\n",
    "\n",
    "# Alternatively, you can directly use the current device without explicitly creating it\n",
    "tensor_on_current_device = torch.tensor([1, 2, 3]).to(device=current_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.6936251451619984\n",
      "Cost after iteration 100: 0.687809722077625\n",
      "Cost after iteration 200: 0.6841938090937976\n",
      "Cost after iteration 300: 0.6819354593658423\n",
      "Cost after iteration 400: 0.6805177357333827\n",
      "Cost after iteration 500: 0.6796230394755856\n",
      "Cost after iteration 600: 0.6790555190242802\n",
      "Cost after iteration 700: 0.6786937804134245\n",
      "Cost after iteration 800: 0.6784621558914391\n",
      "Cost after iteration 900: 0.6783132124880311\n",
      "Predictions: [[0.65181593 0.59161964 0.51523502 0.60735181 0.50964031 0.5207633\n",
      "  0.62791032 0.54888811 0.66498369 0.58999598]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data[0:2, :]\n",
    "print(data.shape)\n",
    "np.random.shuffle(data)\n",
    "print(data.shape)\n",
    "print(data[0:70000].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming 'data' is your NumPy array\n",
    "data = np.array([[1, 2, 3],\n",
    "                 [4, 5, 6],\n",
    "                 [7, 8, 9]])\n",
    "\n",
    "# Shuffle the array along the first axis (rows)\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Now 'data' is shuffled by rows\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1/3]: 100%|██████████| 3/3 [00:03<00:00,  1.00s/it]\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2/3]: 100%|██████████| 3/3 [00:03<00:00,  1.00s/it]\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3/3]: 100%|██████████| 3/3 [00:03<00:00,  1.00s/it]\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "for i in range(3):\n",
    "    loop = tqdm(range(3), leave=True, total=3)\n",
    "    for j in loop:\n",
    "        time.sleep(1)\n",
    "        loop.set_description(f\"[{i+1}/{3}]\")\n",
    "        # loop.set_postfix(batch=batch, loss=loss.item())\n",
    "    for k in tqdm(range(3), leave=True, total=3):\n",
    "        time.sleep(1)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 11):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "# Define a simple neural network and loss function\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Function to train the model and visualize the loss dynamically\n",
    "def train_and_visualize_live(model, criterion, optimizer, num_epochs=100):\n",
    "    # Initialize the plot\n",
    "    plt.ion()  # Turn on interactive mode\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Training Loss vs. Epochs')\n",
    "    line, = ax.plot([], [], marker='o', color='b')\n",
    "    plt.show()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Generate dummy data\n",
    "        inputs = torch.rand((100, 1))\n",
    "        targets = 3 * inputs + 2 + 0.1 * torch.randn_like(inputs)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the plot dynamically\n",
    "        line.set_xdata(range(epoch + 1))\n",
    "        line.set_ydata([loss.item()] * (epoch + 1))\n",
    "        ax.relim()\n",
    "        ax.autoscale_view()\n",
    "\n",
    "        # Update the plot in the notebook\n",
    "        clear_output(wait=True)\n",
    "        display(fig)\n",
    "\n",
    "        # Pause to visualize\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    # Turn off interactive mode after training\n",
    "    plt.ioff()\n",
    "    plt.show()\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = SimpleModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model and visualize the loss dynamically\n",
    "train_and_visualize_live(model, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Define a simple neural network and loss function\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Function to train the model and visualize the loss dynamically\n",
    "def train_and_visualize(model, criterion, optimizer, num_epochs=100, video_path='loss_video.mp4'):\n",
    "    # Set up video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(video_path, fourcc, 2.0, (800, 600))\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Generate dummy data\n",
    "        inputs = torch.rand((100, 1))\n",
    "        targets = 3 * inputs + 2 + 0.1 * torch.randn_like(inputs)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Plot and save frame\n",
    "        plt.plot(epoch, loss.item(), marker='o', color='b')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss vs. Epochs')\n",
    "        plt.savefig('temp.png')\n",
    "        img = cv2.imread('temp.png')\n",
    "        out.write(img)\n",
    "\n",
    "    # Release video writer\n",
    "    out.release()\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = SimpleModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model and visualize the loss dynamically\n",
    "train_and_visualize(model, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
